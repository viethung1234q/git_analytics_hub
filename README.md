```
 $$$$$$\  $$\   $$\            $$$$$$\                      $$\             $$\     $$\                             $$\   $$\           $$\       
$$  __$$\ \__|  $$ |          $$  __$$\                     $$ |            $$ |    \__|                            $$ |  $$ |          $$ |      
$$ /  \__|$$\ $$$$$$\         $$ /  $$ |$$$$$$$\   $$$$$$\  $$ |$$\   $$\ $$$$$$\   $$\  $$$$$$$\  $$$$$$$\         $$ |  $$ |$$\   $$\ $$$$$$$\  
$$ |$$$$\ $$ |\_$$  _|$$$$$$\ $$$$$$$$ |$$  __$$\  \____$$\ $$ |$$ |  $$ |\_$$  _|  $$ |$$  _____|$$  _____|$$$$$$\ $$$$$$$$ |$$ |  $$ |$$  __$$\ 
$$ |\_$$ |$$ |  $$ |  \______|$$  __$$ |$$ |  $$ | $$$$$$$ |$$ |$$ |  $$ |  $$ |    $$ |$$ /      \$$$$$$\  \______|$$  __$$ |$$ |  $$ |$$ |  $$ |
$$ |  $$ |$$ |  $$ |$$\       $$ |  $$ |$$ |  $$ |$$  __$$ |$$ |$$ |  $$ |  $$ |$$\ $$ |$$ |       \____$$\         $$ |  $$ |$$ |  $$ |$$ |  $$ |
\$$$$$$  |$$ |  \$$$$  |      $$ |  $$ |$$ |  $$ |\$$$$$$$ |$$ |\$$$$$$$ |  \$$$$  |$$ |\$$$$$$$\ $$$$$$$  |        $$ |  $$ |\$$$$$$  |$$$$$$$  |
 \______/ \__|   \____/       \__|  \__|\__|  \__| \_______|\__| \____$$ |   \____/ \__| \_______|\_______/         \__|  \__| \______/ \_______/ 
                                                                $$\   $$ |                                                                        
                                                                \$$$$$$  |                                                                        
                                                                 \______/                                                                         
```
## Description
Git-Analytics-Hub is a tool that automates the collection, processing, and storage of GitHub Archive data. It runs hourly to download raw data from GitHub Archive. Then, it processes the raw data and aggregates the processed data, both using DuckDB. All the data is stored in MinIO.

## Architecture
We will use muti-tier architecture - the **Medallion Architecture** - which is a data lake design pattern that organises data into three zones:
- **Bronze Zone**: Containing raw, unprocessed data ingested from various sources.
- **Silver Zone**: Containing cleaned, conformed and potentially modeled data.
- **Gold Zone**: Containing aggregated and curated data ready for reporting, dashboards, and advanced analytics.

![Architecture](./images/medallion_architecture.png)

Git-Analytics-Hub follows this architecture, with each layer:
- **Bronze Layer**: Stores raw data from [GitHub Archive](https://www.gharchive.org/) data in Minio.
- **Silver Layer**: Processes raw data into structured tables in DuckDB.
- **Gold Layer**: Aggregates and exports data as `.parquet` files for downstream use.

The workflow is managed using Apache Airflow, ensuring automated and scheduled data processing.

## Installation

### Prerequisites
Ensure you have the following installed:
- **Docker** (tested with version `28.0.1`)
- **Docker Compose** (tested with version `2.33.1`)
- **Python** (tested with version `3.10.15`)

### Setup
Clone the repository and navigate to the project directory:
```sh
git clone https://github.com/viethung1234q/git_analytics_hub.git
cd git_analytics_hub
```

## Usage

### First-time setup:
Initialize Airflow before running the services:
```sh
docker compose up airflow-init
```
Then, start the services:
```sh
docker compose up --build -d
```

### Subsequent runs:
For all future runs, simply use:
```sh
docker compose up --build -d
```

## Configuration

### Environment Variables
Environment variables are defined in the `.env` file:
```ini
AIRFLOW_UID=1000
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=false
AIRFLOW_PROJ_DIR=./airflow
AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Ho_Chi_Minh
```

### Project Configuration
Project-specific configurations are stored in `git_analytics_hub/src/config.ini`:
```ini
[minio]
access_key = minioadmin
secret_key = minioadmin
...
```

## Additional Notes
- Airflow DAGs are configured to run hourly, processing GitHub Archive data.
- Minio is used as object storage for raw and processed data.
- DuckDB serves as the query engine for data transformations.

For detailed documentation and contributions, refer to the official repository.


Minio
DuckDB
Gharchive
Hourly fetched data

Incrementally collecting the Github Archive datasets, which provide a full record of GitHub activities for public repositories, and enabling analytics on top of that data.

We will use Medallion architecture/multi-tier architecture for this project. 
The Medallion architecture is a data lake design pattern that organises data into three zones:
- Bronze Zone: Containing raw, unprocessed data ingested from various sources.
- Silver Zone: Containing cleaned, conformed and potentially modeled data.
- Gold Zone: Containing aggregated and curated data ready for reporting, dashboards, and advanced analytics.


LÃ½ do nÃªn cÃ i Airflow báº±ng Docker + Docker Compose trÃªn WSL 2 thay vÃ¬ cÃ i trá»±c tiáº¿p báº±ng pip lÃ  vÃ¬:

1. Dá»… dÃ ng quáº£n lÃ½ vÃ  triá»ƒn khai
Táº¥t cáº£ thÃ nh pháº§n cá»§a Airflow (Scheduler, Webserver, Database) Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i trong Docker container, khÃ´ng cáº§n cÃ i Ä‘áº·t thá»§ cÃ´ng PostgreSQL/MySQL hay cÃ¡c dependency khÃ¡c.
Khi cáº§n di chuyá»ƒn Airflow sang mÃ´i trÆ°á»ng khÃ¡c (server, cloud), chá»‰ cáº§n pull Docker image vá» vÃ  cháº¡y.
2. TrÃ¡nh xung Ä‘á»™t mÃ´i trÆ°á»ng
Airflow cÃ³ ráº¥t nhiá»u dependency (SQLAlchemy, Flask, Celery...) dá»… gÃ¢y lá»—i náº¿u cÃ i báº±ng pip trÃªn mÃ´i trÆ°á»ng local.
Náº¿u báº¡n cÃ i Airflow trá»±c tiáº¿p trÃªn WSL 2, cÃ³ thá»ƒ xáº£y ra xung Ä‘á»™t giá»¯a Python version hoáº·c package dependency vá»›i cÃ¡c thÆ° viá»‡n khÃ¡c.
3. Dá»… dÃ ng cáº­p nháº­t vÃ  rollback
Vá»›i Docker, náº¿u cÃ³ báº£n cáº­p nháº­t Airflow, chá»‰ cáº§n thay Ä‘á»•i image version vÃ  restart container, khÃ´ng lo vá» lá»—i do cÃ i Ä‘áº·t thá»§ cÃ´ng.
Náº¿u cÃ³ lá»—i, chá»‰ cáº§n pull láº¡i image cÅ© lÃ  xong.
4. TÃ­ch há»£p dá»… dÃ ng vá»›i Minio vÃ  DuckDB
Báº¡n Ä‘ang dÃ¹ng Minio vÃ  DuckDB, cÃ³ thá»ƒ cháº¡y chÃºng trong cÃ¡c container riÃªng vÃ  káº¿t ná»‘i vá»›i Airflow qua Docker Compose mÃ  khÃ´ng cáº§n cÃ i Ä‘áº·t riÃªng láº».

-> Káº¿t luáº­n: CÃ i Ä‘áº·t báº±ng Docker giÃºp báº¡n dá»… quáº£n lÃ½, dá»… má»Ÿ rá»™ng, khÃ´ng lo lá»—i dependency, phÃ¹ há»£p Ä‘á»ƒ triá»ƒn khai trÃªn báº¥t ká»³ há»‡ thá»‘ng nÃ o. ðŸš€


curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.5/docker-compose.yaml'
mkdir -p ./airflow
mkdir -p ./airflow/dags ./airflow/logs ./airflow/config ./airflow/plugins
echo -e "AIRFLOW_UID=$(id -u)" > .env
docker compose up airflow-init
docker compose up --build -d
